Note: We consider only 1-D dataset to empirical analysis

Stable Hyperparameters: confidence_threshold, pthv 
Sensitive Hyperparameters: sigma_ini, tau
Prior Probabilities generated are not correct in some cases.

Case-I: Checking the efficiency corresponding to the number of generated samples
-> As the samples increase, the covariance values decrease as the model becomes increasingly more definite.
-> The objective here is to basically check if the model is updating beliefs correctly according to new data being fitted into the model.

Case-II: Checking the efficiency corresponding to the hyperparameters
( We assume that the parameters of the source mixture do not significantly overlap and each datapoint generated has a significant distinction to the mixture component it belongs to )
-> The mean is not affected. The offset is correctly determined in almost every case until the number of components are correctly determined.
-> Sometimes the model fails to capture the correct approximation of prior probabilities in the classical case, which depends upon novelty threshold ( tau ).
-> The model is too sensitive on the sigma_ini parameter to detemine the covariance. It affects covariance too much if we set it too high or too low.
-> The pthv component is not properly extrapolated in some cases. It reduces the number of components to a great extent in some samples.
-> The confidence threshold if set above 0.95 usually estimates correct mixture and is stable compared to other hyperparameters.

Case-III: Checking the efficiency corresponding to the parameters of the Gaussian Mixtures
-> As we begin to merge our cluster mixtures together, we need to increase the novelty threshold to incorporate getting more number of components than what they originally look like
-> sigma_ini is a major initialization issue to get a correct sense of covariance values and it also has a significant effect on the number of components
-> We need to have a different settings of hyperparameter for different set of structure of source mixtures.
-> To get a robust estimate of correct fit, we need to extrapolate these hyperparameters on a large set of dataset and fit them accordingly
-> If the structure of 2 gaussians is quite different from each, setting up one hyperparameter corresponding to one gaussian would be compromising the setting of the other

Room for improvement:
-> We do not need to keep hyperparameter's value fixed for all gaussian. 
  Approach-I:  Make a set of hyperparameter for each gaussian component
  Approach-II: Modify the equation form such that it also takes into account the current prior probability and covariance of the component
-> Implement and compare hyperparameter optimization techniques so as to get an optimal set of hyperparameters to achieve the task
  Approach-I: Use tree parzen estimators to achieve an optimal set of hyperparameters, but the time complexity is too high.
-> Instead of just trying to eliminate the spurious components which would inherently result in a loss of data, try to merge them with other gaussian components
  Approach-I: Create a set of potential clusters. For every datapoint which comes, compare using the same tau criterion. If the number of datapoint get above some threshold, include that in the gaussian mixtures.
